"""
Unified LLM Provider for Archon
Supports multiple LLM providers: Ollama, OpenRouter, and OpenAI
"""
import logging
import os
import sys
from typing import Optional, Dict, Any
from dataclasses import dataclass

# Ajouter le r√©pertoire utils au path pour les imports
current_dir = os.path.dirname(os.path.abspath(__file__))
parent_dir = os.path.dirname(current_dir)
utils_dir = os.path.join(parent_dir, "utils")
sys.path.insert(0, utils_dir)

try:
    from utils import get_env_var, get_current_profile, write_to_log
except ImportError:
    # Fallback si l'import √©choue
    def get_env_var(var_name: str, profile: Optional[str] = None) -> Optional[str]:
        return os.environ.get(var_name)
    
    def get_current_profile() -> str:
        return "openrouter"
    
    def write_to_log(message: str):
        print(f"[LOG] {message}")

logger = logging.getLogger(__name__)

@dataclass
class LLMConfig:
    """Configuration for LLM provider"""
    provider: str
    api_key: Optional[str] = None
    base_url: Optional[str] = None
    reasoner_model: str = "tinyllama:1.1b"
    primary_model: str = "tinyllama:1.1b"
    coder_model: Optional[str] = None
    advisor_model: Optional[str] = None

class LLMProvider:
    """Unified LLM Provider that supports multiple backends"""
    
    def __init__(self):
        self.config: Optional[LLMConfig] = None
        self._initialize_from_profile()
    
    def _initialize_from_profile(self):
        """Initialize provider configuration from current profile"""
        try:
            current_profile = get_current_profile()
            write_to_log(f"üîß Initializing LLM Provider with profile: {current_profile}")
            
            # Get provider type
            provider = get_env_var("LLM_PROVIDER", current_profile)
            if not provider:
                # Try alternative names
                provider = get_env_var("PROVIDER", current_profile)
            
            if not provider:
                logger.warning("‚ùå No LLM provider specified, defaulting to OpenRouter")
                provider = "OpenRouter"
            
            # Normalize provider name
            provider = provider.lower()
            
            if provider == "ollama":
                self._initialize_ollama()
            elif provider == "openrouter":
                self._initialize_openrouter()
            elif provider == "openai":
                self._initialize_openai()
            else:
                logger.error(f"‚ùå Unsupported provider: {provider}")
                raise ValueError(f"Unsupported LLM provider: {provider}")
            
            logger.info(f"‚úÖ LLM Provider '{provider}' initialized successfully")
            write_to_log(f"‚úÖ LLM Provider '{provider}' initialized successfully")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize LLM provider: {e}")
            write_to_log(f"‚ùå Failed to initialize LLM provider: {e}")
            # Create a fallback config to prevent crashes
            self.config = LLMConfig(
                provider="openrouter",
                reasoner_model="deepseek/deepseek-chat-v3-0324:free",
                primary_model="moonshotai/kimi-k2:free"
            )
    
    def _initialize_ollama(self):
        """Initialize Ollama configuration"""
        base_url = get_env_var("OLLAMA_BASE_URL") or "http://localhost:11434"
        reasoner_model = get_env_var("REASONER_MODEL") or "tinyllama:1.1b"
        primary_model = get_env_var("PRIMARY_MODEL") or "tinyllama:1.1b"
        
        self.config = LLMConfig(
            provider="ollama",
            base_url=base_url,
            reasoner_model=reasoner_model,
            primary_model=primary_model
        )
        
        logger.info(f"ü¶ô Ollama configured: {base_url}")
        logger.info(f"üß† Reasoner model: {reasoner_model}")
        logger.info(f"‚ö° Primary model: {primary_model}")
    
    def _initialize_openrouter(self):
        """Initialize OpenRouter configuration"""
        api_key = get_env_var("OPENROUTER_API_KEY")
        if not api_key:
            logger.warning("‚ö†Ô∏è OPENROUTER_API_KEY not found, trying LLM_API_KEY")
            api_key = get_env_var("LLM_API_KEY")
        
        if not api_key:
            raise ValueError("‚ùå OpenRouter API key is required but not found")
        
        reasoner_model = get_env_var("REASONER_MODEL") or "deepseek/deepseek-chat-v3-0324:free"
        primary_model = get_env_var("PRIMARY_MODEL") or "moonshotai/kimi-k2:free"
        coder_model = get_env_var("CODER_MODEL") or "qwen/qwen3-coder:free"
        advisor_model = get_env_var("ADVISOR_MODEL") or "meta-llama/llama-3.1-8b-instruct:free"
        
        self.config = LLMConfig(
            provider="openrouter",
            api_key=api_key,
            base_url="https://openrouter.ai/api/v1",
            reasoner_model=reasoner_model,
            primary_model=primary_model,
            coder_model=coder_model,
            advisor_model=advisor_model
        )
        
        # Mask API key for logging
        masked_key = api_key[:6] + "*****" + api_key[-4:] if len(api_key) > 10 else "***"
        logger.info(f"üîë OpenRouter configured with key: {masked_key}")
        logger.info(f"üß† Reasoner model: {reasoner_model}")
        logger.info(f"‚ö° Primary model: {primary_model}")
    
    def _initialize_openai(self):
        """Initialize OpenAI configuration"""
        api_key = get_env_var("OPENAI_API_KEY")
        if not api_key:
            logger.warning("‚ö†Ô∏è OPENAI_API_KEY not found, trying LLM_API_KEY")
            api_key = get_env_var("LLM_API_KEY")
        
        if not api_key:
            raise ValueError("‚ùå OpenAI API key is required but not found")
        
        reasoner_model = get_env_var("REASONER_MODEL") or "gpt-4.1-nano"
        primary_model = get_env_var("PRIMARY_MODEL") or "gpt-4.1-nano"
        
        self.config = LLMConfig(
            provider="openai",
            api_key=api_key,
            base_url="https://api.openai.com/v1",
            reasoner_model=reasoner_model,
            primary_model=primary_model
        )
        
        # Mask API key for logging
        masked_key = api_key[:6] + "*****" + api_key[-4:] if len(api_key) > 10 else "***"
        logger.info(f"üîë OpenAI configured with key: {masked_key}")
        logger.info(f"üß† Reasoner model: {reasoner_model}")
        logger.info(f"‚ö° Primary model: {primary_model}")
    
    def reload_config(self):
        """Reload configuration from current profile"""
        self._initialize_from_profile()
    
    def get_model_for_agent(self, agent_type: str) -> str:
        """Get the appropriate model for a specific agent type"""
        if not self.config:
            raise ValueError("LLM Provider not initialized")
        
        if agent_type == "reasoner":
            return self.config.reasoner_model
        elif agent_type == "coder":
            return self.config.coder_model or self.config.primary_model
        elif agent_type == "advisor":
            return self.config.advisor_model or self.config.primary_model
        else:
            return self.config.primary_model

def initialize_llm_provider() -> LLMProvider:
    """Initialize and return a new LLM provider instance"""
    return LLMProvider()

# Global instance
llm_provider: Optional[LLMProvider] = None

def get_llm_provider() -> LLMProvider:
    """Get or create the global LLM provider instance"""
    global llm_provider
    if llm_provider is None:
        llm_provider = initialize_llm_provider()
    return llm_provider

# Initialize on import
try:
    llm_provider = initialize_llm_provider()
    if llm_provider and llm_provider.config:
        logger.info(f"üöÄ LLM Provider initialized with {llm_provider.config.provider}")
    else:
        logger.warning("‚ö†Ô∏è LLM Provider initialized but config is None")
except Exception as e:
    logger.error(f"‚ùå Failed to initialize LLM provider on import: {e}")
    llm_provider = None
